{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d596ac4",
   "metadata": {},
   "source": [
    "# Loan Default Prediction 3 - Modeling 1\n",
    "\n",
    "\n",
    "We’ll use two models for baseline evaluation:\n",
    "\n",
    "Logistic Regression (interpretable and works well for binary classification).\n",
    "Gradient Boosting (e.g., XGBoost or LightGBM) for a stronger, non-linear model.\n",
    "\n",
    "At the end we will compare the two. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57d6023",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84202912",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score, roc_curve, accuracy_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb8b6311",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>person_emp_length</th>\n",
       "      <th>loan_amnt</th>\n",
       "      <th>loan_int_rate</th>\n",
       "      <th>loan_status</th>\n",
       "      <th>loan_percent_income</th>\n",
       "      <th>cb_person_cred_hist_length</th>\n",
       "      <th>loan_intent_EDUCATION</th>\n",
       "      <th>loan_intent_HOMEIMPROVEMENT</th>\n",
       "      <th>loan_intent_MEDICAL</th>\n",
       "      <th>loan_intent_PERSONAL</th>\n",
       "      <th>...</th>\n",
       "      <th>loan_grade_B</th>\n",
       "      <th>loan_grade_C</th>\n",
       "      <th>loan_grade_D</th>\n",
       "      <th>loan_grade_E</th>\n",
       "      <th>loan_grade_F</th>\n",
       "      <th>loan_grade_G</th>\n",
       "      <th>person_home_ownership_OTHER</th>\n",
       "      <th>person_home_ownership_OWN</th>\n",
       "      <th>person_home_ownership_RENT</th>\n",
       "      <th>cb_person_default_on_file_Y</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>person_income</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9600</th>\n",
       "      <td>5.0</td>\n",
       "      <td>1000</td>\n",
       "      <td>11.14</td>\n",
       "      <td>0</td>\n",
       "      <td>0.10</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9600</th>\n",
       "      <td>1.0</td>\n",
       "      <td>5500</td>\n",
       "      <td>12.87</td>\n",
       "      <td>1</td>\n",
       "      <td>0.57</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65500</th>\n",
       "      <td>4.0</td>\n",
       "      <td>35000</td>\n",
       "      <td>15.23</td>\n",
       "      <td>1</td>\n",
       "      <td>0.53</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54400</th>\n",
       "      <td>8.0</td>\n",
       "      <td>35000</td>\n",
       "      <td>14.27</td>\n",
       "      <td>1</td>\n",
       "      <td>0.55</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9900</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2500</td>\n",
       "      <td>7.14</td>\n",
       "      <td>1</td>\n",
       "      <td>0.25</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               person_emp_length  loan_amnt  loan_int_rate  loan_status  \\\n",
       "person_income                                                             \n",
       "9600                         5.0       1000          11.14            0   \n",
       "9600                         1.0       5500          12.87            1   \n",
       "65500                        4.0      35000          15.23            1   \n",
       "54400                        8.0      35000          14.27            1   \n",
       "9900                         2.0       2500           7.14            1   \n",
       "\n",
       "               loan_percent_income  cb_person_cred_hist_length  \\\n",
       "person_income                                                    \n",
       "9600                          0.10                           2   \n",
       "9600                          0.57                           3   \n",
       "65500                         0.53                           2   \n",
       "54400                         0.55                           4   \n",
       "9900                          0.25                           2   \n",
       "\n",
       "               loan_intent_EDUCATION  loan_intent_HOMEIMPROVEMENT  \\\n",
       "person_income                                                       \n",
       "9600                            True                        False   \n",
       "9600                           False                        False   \n",
       "65500                          False                        False   \n",
       "54400                          False                        False   \n",
       "9900                           False                        False   \n",
       "\n",
       "               loan_intent_MEDICAL  loan_intent_PERSONAL  ...  loan_grade_B  \\\n",
       "person_income                                             ...                 \n",
       "9600                         False                 False  ...          True   \n",
       "9600                          True                 False  ...         False   \n",
       "65500                         True                 False  ...         False   \n",
       "54400                         True                 False  ...         False   \n",
       "9900                         False                 False  ...         False   \n",
       "\n",
       "               loan_grade_C  loan_grade_D  loan_grade_E  loan_grade_F  \\\n",
       "person_income                                                           \n",
       "9600                  False         False         False         False   \n",
       "9600                   True         False         False         False   \n",
       "65500                  True         False         False         False   \n",
       "54400                  True         False         False         False   \n",
       "9900                  False         False         False         False   \n",
       "\n",
       "               loan_grade_G  person_home_ownership_OTHER  \\\n",
       "person_income                                              \n",
       "9600                  False                        False   \n",
       "9600                  False                        False   \n",
       "65500                 False                        False   \n",
       "54400                 False                        False   \n",
       "9900                  False                        False   \n",
       "\n",
       "               person_home_ownership_OWN  person_home_ownership_RENT  \\\n",
       "person_income                                                          \n",
       "9600                                True                       False   \n",
       "9600                               False                       False   \n",
       "65500                              False                        True   \n",
       "54400                              False                        True   \n",
       "9900                                True                       False   \n",
       "\n",
       "               cb_person_default_on_file_Y  \n",
       "person_income                               \n",
       "9600                                 False  \n",
       "9600                                 False  \n",
       "65500                                False  \n",
       "54400                                 True  \n",
       "9900                                 False  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/data_for_model.csv\", index_col=0)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19af7710",
   "metadata": {},
   "source": [
    "### Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "472238bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are doing an 80/20 split\n",
    "\n",
    "X = df.drop(columns=['loan_status'])\n",
    "y = df['loan_status'] \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=99)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4ac020",
   "metadata": {},
   "source": [
    "## Scale Numerical Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "034dde66",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa24de5",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d7bc8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(random_state=42)\n",
    "log_reg.fit(X_train_scaled, y_train)\n",
    "y_pred_lr = log_reg.predict(X_test_scaled)\n",
    "y_pred_lr_proba = log_reg.predict_proba(X_test_scaled)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05402dc3",
   "metadata": {},
   "source": [
    "#### Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9537645f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.95      0.92      4601\n",
      "           1       0.76      0.59      0.66      1291\n",
      "\n",
      "    accuracy                           0.87      5892\n",
      "   macro avg       0.83      0.77      0.79      5892\n",
      "weighted avg       0.86      0.87      0.86      5892\n",
      "\n",
      "AUC-ROC: 0.87\n"
     ]
    }
   ],
   "source": [
    "print(\"Logistic Regression Performance:\")\n",
    "print(classification_report(y_test, y_pred_lr))\n",
    "print(f\"AUC-ROC: {roc_auc_score(y_test, y_pred_lr_proba):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2291efb4",
   "metadata": {},
   "source": [
    "Precision:\n",
    "Class 0 (Non-default): 0.89 - Out of all the loans predicted as non-default, 89% were actually non-default.\n",
    "Class 1 (Default): 0.76 - Out of all loans predicted as default, 76% were actually default.\n",
    "\n",
    "Recall:\n",
    "Class 0 (Non-default): 0.95 - The model identified 95% of the actual non-default loans correctly.\n",
    "Class 1 (Default): 0.59 - The model only identified 59% of the actual default loans correctly, which indicates some room for improvement in detecting defaults.\n",
    "\n",
    "F1-Score:\n",
    "Class 0 (Non-default): 0.92 - Strong balance of precision and recall for non-defaults.\n",
    "Class 1 (Default): 0.66 - Lower F1-score for defaults, reflecting lower recall for this class.\n",
    "\n",
    "Accuracy: 0.87 (87%) - The model correctly classified 87% of loans overall.\n",
    "\n",
    "AUC-ROC: 0.87 - Indicates good separability between the two classes, suggesting the model is performing well at distinguishing between defaults and non-defaults.\n",
    "\n",
    "The recall for defaults (Class 1) is low (0.59), meaning the model misses a significant number of actual defaults.\n",
    "This is expected since defaults (Class 1) are likely the minority class in the dataset, and Logistic Regression tends to favor the majority class (Non-default, Class 0).\n",
    "Good Performance for Non-defaults. Precision, recall, and F1-score for non-defaults (Class 0) are strong, which is helpful for identifying reliable borrowers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449b5521",
   "metadata": {},
   "source": [
    "# Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49446e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_clf = GradientBoostingClassifier(random_state=42)\n",
    "gb_clf.fit(X_train, y_train)\n",
    "y_pred_gb = gb_clf.predict(X_test)\n",
    "y_pred_gb_proba = gb_clf.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c914156",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5763e5dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Gradient Boosting Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.98      0.95      4601\n",
      "           1       0.91      0.69      0.79      1291\n",
      "\n",
      "    accuracy                           0.92      5892\n",
      "   macro avg       0.92      0.84      0.87      5892\n",
      "weighted avg       0.92      0.92      0.91      5892\n",
      "\n",
      "AUC-ROC: 0.92\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nGradient Boosting Performance:\")\n",
    "print(classification_report(y_test, y_pred_gb))\n",
    "print(f\"AUC-ROC: {roc_auc_score(y_test, y_pred_gb_proba):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed75e75",
   "metadata": {},
   "source": [
    "Precision:\n",
    "Class 0 (Non-default): 0.92 - Out of all loans predicted as non-default, 92% were actually non-default.\n",
    "Class 1 (Default): 0.91 - Out of all loans predicted as default, 91% were actually default. This is a significant improvement over Logistic Regression (0.76 for Class 1 precision).\n",
    "\n",
    "Recall:\n",
    "Class 0 (Non-default): 0.98 - The model identified 98% of the actual non-default loans correctly, which is very strong.\n",
    "Class 1 (Default): 0.69 - The model identified 69% of the actual default loans correctly. This is an improvement over Logistic Regression (0.59 recall for Class 1).\n",
    "\n",
    "F1-Score:\n",
    "Class 0 (Non-default): 0.95 - Excellent balance of precision and recall for non-defaults.\n",
    "Class 1 (Default): 0.79 - Improved F1-score for defaults compared to Logistic Regression (0.66).\n",
    "\n",
    "Accuracy: 0.92 (92%) - The model correctly classified 92% of loans overall, which is higher than Logistic Regression (87%).\n",
    "\n",
    "AUC-ROC: 0.92 - Indicates excellent separability between the two classes, showing that the model is effectively distinguishing defaults from non-defaults."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfe0cc0",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Precision (0.91) and recall (0.69) for defaults are significantly better than Logistic Regression, making this model more effective at identifying high-risk borrowers.\n",
    "Overall Performance:\n",
    "\n",
    "Gradient Boosting has better overall accuracy, AUC-ROC, and F1-scores for both classes compared to Logistic Regression.\n",
    "It's capturing the non-linear relationships in the data, which Logistic Regression couldn't.\n",
    "Class Imbalance Impact:\n",
    "\n",
    "While Gradient Boosting performs better on the minority class (defaults), recall (0.69) for Class 1 could still be improved further."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9a3a5d",
   "metadata": {},
   "source": [
    "### Next Step - Ways to improve the models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0642e32",
   "metadata": {},
   "source": [
    "**Logistic Agression**\n",
    "The model only identified 59% of the actual default loans correctly, this is expected since defaults (Class 1) are likely the minority class in the dataset, and Logistic Regression tends to favor the majority class (Non-default, Class 0). \n",
    "\n",
    "To address this we will use techniques to handle class imbalance and improve recall for defaults. We will consider: \n",
    "\n",
    "- Class Weights: By assigning a higher weight to the minority class.\n",
    "- Oversampling - We can consider using SMOTE (Synthetic Minority Oversampling Technique) to generate more synthetic examples for the default class.\n",
    "- Undersampling: Reduce the majority class (non-default) samples.\n",
    "\n",
    " Addition to class imbalance, we can also perform hyperparameter tuning using GridSearchCV or RandomizedSearchCV. We will first focus on the class imbalance before further tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d599fa1e",
   "metadata": {},
   "source": [
    "**Gradient Boosting** This model has better overall accuracy, but we can still see how well it performs after handling class imbalance. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9880b8",
   "metadata": {},
   "source": [
    "# Approach\n",
    "We will be comparing the 3 different methods and seeing which one is the best method. \n",
    "##### 1. Class Weights\n",
    "Adjust the model to assign a higher weight to the minority class (loan_status=1) to penalize the model more for misclassifying defaults.\n",
    "\n",
    "**Implementation:**\n",
    "Use class_weight='balanced'\n",
    "\n",
    "##### 2. Oversampling with SMOTE\n",
    "We will use SMOTE (Synthetic Minority Oversampling Technique) to create synthetic examples for the minority class.\n",
    "\n",
    "**Implementation:**\n",
    "Use the SMOTE class from imblearn to oversample the training data.\n",
    "We will need to be careful to only apply SMOTE to the training set to avoid data leakage.\n",
    "\n",
    "##### 3. Undersampling\n",
    "Randomly remove samples from the majority class to balance the dataset.\n",
    "\n",
    "**Implementation:**\n",
    "Use the RandomUnderSampler class from imblearn.\n",
    "Like SMOTE, we need to make sure we are applying undersampling only to the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1ec6f2",
   "metadata": {},
   "source": [
    "## Class Imbalance in Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b97bbd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression with Class Weights\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.80      0.86      4601\n",
      "           1       0.53      0.78      0.63      1291\n",
      "\n",
      "    accuracy                           0.80      5892\n",
      "   macro avg       0.73      0.79      0.74      5892\n",
      "weighted avg       0.84      0.80      0.81      5892\n",
      "\n",
      "AUC-ROC: 0.872\n",
      "\n",
      "Logistic Regression with SMOTE\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.86      0.88      4601\n",
      "           1       0.58      0.69      0.63      1291\n",
      "\n",
      "    accuracy                           0.82      5892\n",
      "   macro avg       0.74      0.77      0.75      5892\n",
      "weighted avg       0.83      0.82      0.83      5892\n",
      "\n",
      "AUC-ROC: 0.848\n",
      "\n",
      "Logistic Regression with Undersampling\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.79      0.85      4601\n",
      "           1       0.51      0.76      0.61      1291\n",
      "\n",
      "    accuracy                           0.78      5892\n",
      "   macro avg       0.71      0.78      0.73      5892\n",
      "weighted avg       0.83      0.78      0.80      5892\n",
      "\n",
      "AUC-ROC: 0.857\n"
     ]
    }
   ],
   "source": [
    "# Helper function to evaluate Logistic Regression\n",
    "def evaluate_model(model, X_train, X_test, y_train, y_test):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(f\"AUC-ROC: {roc_auc_score(y_test, y_pred_proba):.3f}\")\n",
    "\n",
    "\n",
    "\n",
    "# 1. Logistic Regression with Class Weights\n",
    "print(\"Logistic Regression with Class Weights\")\n",
    "weighted_model = LogisticRegression(class_weight='balanced', random_state=42, solver='liblinear')\n",
    "evaluate_model(weighted_model, X_train, X_test, y_train, y_test)\n",
    "\n",
    "# 2. Logistic Regression with SMOTE\n",
    "print(\"\\nLogistic Regression with SMOTE\")\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "oversampled_model = LogisticRegression(random_state=42, solver='liblinear')\n",
    "evaluate_model(oversampled_model, X_train_smote, X_test, y_train_smote, y_test)\n",
    "\n",
    "# 3. Logistic Regression with Undersampling\n",
    "print(\"\\nLogistic Regression with Undersampling\")\n",
    "undersampler = RandomUnderSampler(random_state=42)\n",
    "X_train_under, y_train_under = undersampler.fit_resample(X_train, y_train)\n",
    "undersampled_model = LogisticRegression(random_state=42, solver='liblinear')\n",
    "evaluate_model(undersampled_model, X_train_under, X_test, y_train_under, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf3cb7e",
   "metadata": {},
   "source": [
    "## Observations\n",
    "### Class Weights\n",
    "\n",
    "**Strengths:**\n",
    "Best recall for defaults (78%), meaning the model identified most default cases.\n",
    "Balanced precision and recall resulted in the highest AUC-ROC (0.872), indicating strong overall performance.\n",
    "\n",
    "**Weaknesses:**\n",
    "Lower precision (53%) means more false positives, but this is acceptable when the goal is to minimize missed defaults.\n",
    "\n",
    "### SMOTE (Oversampling)\n",
    "**Strengths:**\n",
    "Better precision (58%) compared to class weights, reducing false positives.\n",
    "Maintains good recall (69%) and accuracy (82%).\n",
    "\n",
    "**Weaknesses:**\n",
    "Slightly lower AUC-ROC (0.848), indicating the model struggles more to separate classes.\n",
    "\n",
    "### Undersampling\n",
    "**Strengths:**\n",
    "Comparable recall (76%) to class weights.\n",
    "Simplifies the dataset by balancing classes.\n",
    "\n",
    "**Weaknesses:**\n",
    "Lowest precision (51%) and F1-score (61%) due to potential overfitting on the reduced dataset.\n",
    "Loses information about non-default patterns by removing majority-class examples.\n",
    "\n",
    "## Which Method is Best?\n",
    "The choice depends on the problem you're solving:\n",
    "\n",
    "##### Use Class Weights:\n",
    "\n",
    "If identifying as many defaults as possible (high recall) is the priority, class weights are the best option.\n",
    "This method balances performance without modifying the dataset.\n",
    "\n",
    "#### Use SMOTE:\n",
    "\n",
    "If reducing false positives (higher precision) is important, SMOTE is a solid choice.\n",
    "It slightly sacrifices recall for better precision and overall accuracy.\n",
    "\n",
    "#### Avoid Undersampling:\n",
    "\n",
    "While it maintains decent recall, undersampling is the least effective method due to its loss of information and lower precision.\n",
    "\n",
    "### Conclusion\n",
    " In this case, Class Weights emerged as the best approach for Logistic Regression, offering a great balance of recall and overall model performance.\n",
    "\n",
    "**In the next step, I’ll apply these techniques to Gradient Boosting to see how well it handles imbalanced data compared to Logistic Regression.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35665b9",
   "metadata": {},
   "source": [
    "## Gradient Boosting with Class Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f6742c83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting with Class Weights\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.98      0.95      4601\n",
      "           1       0.91      0.69      0.79      1291\n",
      "\n",
      "    accuracy                           0.92      5892\n",
      "   macro avg       0.92      0.84      0.87      5892\n",
      "weighted avg       0.92      0.92      0.91      5892\n",
      "\n",
      "AUC-ROC: 0.921\n",
      "\n",
      "Gradient Boosting with SMOTE\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.93      0.93      4601\n",
      "           1       0.76      0.74      0.75      1291\n",
      "\n",
      "    accuracy                           0.89      5892\n",
      "   macro avg       0.84      0.84      0.84      5892\n",
      "weighted avg       0.89      0.89      0.89      5892\n",
      "\n",
      "AUC-ROC: 0.906\n",
      "\n",
      "Gradient Boosting with Undersampling\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.89      0.91      4601\n",
      "           1       0.66      0.79      0.72      1291\n",
      "\n",
      "    accuracy                           0.87      5892\n",
      "   macro avg       0.80      0.84      0.82      5892\n",
      "weighted avg       0.88      0.87      0.87      5892\n",
      "\n",
      "AUC-ROC: 0.921\n"
     ]
    }
   ],
   "source": [
    "# 1. Gradient Boosting with Class Weights\n",
    "print(\"Gradient Boosting with Class Weights\")\n",
    "weighted_model = GradientBoostingClassifier(random_state=42)\n",
    "evaluate_model(weighted_model, X_train, X_test, y_train, y_test)\n",
    "\n",
    "# 2. Gradient Boosting with SMOTE\n",
    "print(\"\\nGradient Boosting with SMOTE\")\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "oversampled_model = GradientBoostingClassifier(random_state=42)\n",
    "evaluate_model(oversampled_model, X_train_smote, X_test, y_train_smote, y_test)\n",
    "\n",
    "# 3. Gradient Boosting with Undersampling\n",
    "print(\"\\nGradient Boosting with Undersampling\")\n",
    "undersampler = RandomUnderSampler(random_state=42)\n",
    "X_train_under, y_train_under = undersampler.fit_resample(X_train, y_train)\n",
    "undersampled_model = GradientBoostingClassifier(random_state=42)\n",
    "evaluate_model(undersampled_model, X_train_under, X_test, y_train_under, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b6a871",
   "metadata": {},
   "source": [
    "## Analysis of Gradient Boosting Results with Class Imbalance Techniques\n",
    "### 1. Gradient Boosting with Class Weights\n",
    "#### Performance Metrics:\n",
    "\n",
    "**Precision (Class 1)**: 0.91 - Excellent precision, meaning 91% of loans predicted as defaults were actually defaults.\n",
    "\n",
    "**Recall (Class 1):** 0.69 - Identified 69% of actual defaults, a strong performance.\n",
    "\n",
    "**F1-Score (Class 1):** 0.79 - Balanced precision and recall.\n",
    "\n",
    "**AUC-ROC:** 0.921 - Outstanding ability to distinguish between defaults and non-defaults.\n",
    "\n",
    "#### Observations:\n",
    "\n",
    "Class weights achieved high precision and recall, striking a good balance.\n",
    "High AUC-ROC indicates the model effectively separates the two classes.\n",
    "\n",
    "### 2. Gradient Boosting with SMOTE (Oversampling)\n",
    "#### Performance Metrics:\n",
    "\n",
    "**Precision (Class 1)**: 0.76 - Lower than class weights, meaning more false positives.\n",
    "**Recall (Class 1):** 0.74 - Improved recall compared to class weights, catching more defaults.\n",
    "**F1-Score (Class 1):** 0.75 - Balanced but slightly lower than class weights.\n",
    "**AUC-ROC:** 0.906 - Slightly lower separability compared to class weights.\n",
    "\n",
    "#### Observations:\n",
    "\n",
    "SMOTE sacrifices precision for better recall, making it a good option when capturing defaults is critical.\n",
    "Slight drop in AUC-ROC reflects the potential noise introduced by synthetic samples.\n",
    "\n",
    "### 3. Gradient Boosting with Undersampling\n",
    "#### Performance Metrics:\n",
    "\n",
    "**Precision (Class 1)**: 0.66 - Lower precision, indicating more false positives.\n",
    "**Recall (Class 1):** 0.79 - The best recall among all techniques, capturing the highest proportion of defaults.\n",
    "**F1-Score (Class 1):** 0.72 - Balanced but slightly lower due to lower precision.\n",
    "**AUC-ROC:**  0.921 - Matches class weights, indicating good class separability.\n",
    "\n",
    "#### Observations:\n",
    "\n",
    "Undersampling is effective at maximizing recall for defaults but struggles with precision.\n",
    "Slightly lower accuracy and F1-score overall due to reduced training data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b92588",
   "metadata": {},
   "source": [
    "### Class Weights:\n",
    "#### Best Overall Option:\n",
    "- High precision (0.91) and balanced recall (0.69).\n",
    "- Best AUC-ROC (0.921), indicating excellent separability.\n",
    "- Suitable for scenarios where false positives are more acceptable than missed defaults.\n",
    "\n",
    "### SMOTE (Oversampling):\n",
    "#### Best for Balanced Recall and Precision:\n",
    "- Improves recall (0.74) while maintaining decent precision (0.76).\n",
    "- Slightly lower AUC-ROC (0.906) due to noise from synthetic samples.\n",
    "- Ideal if you prioritize recall without sacrificing too much precision.\n",
    "\n",
    "### Undersampling:\n",
    "#### Best for Maximizing Recall:\n",
    "- Achieved the highest recall (0.79), making it effective for identifying defaults.\n",
    "- Lower precision (0.66) and accuracy (87%) due to reduced training data.\n",
    "- Suitable when default identification is critical, but false positives are less costly.\n",
    "\n",
    "### Comparison with Logistic Regression\n",
    "Gradient Boosting significantly outperforms Logistic Regression for all three techniques:\n",
    "\n",
    "- Higher Precision and Recall: Gradient Boosting better balances both metrics.\n",
    "- Stronger AUC-ROC: Gradient Boosting achieves better class separability (0.921 vs. 0.872 for class weights).\n",
    "\n",
    "## Conclusion\n",
    "For Gradient Boosting:\n",
    "\n",
    "- Class Weights: Best overall, with the highest AUC-ROC and precision.\n",
    "- SMOTE: A solid choice for slightly better recall without sacrificing much precision.\n",
    "- Undersampling: Use only if maximizing recall is your primary objective."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374d6520",
   "metadata": {},
   "source": [
    "### We will be diving into feature engineering before revisting Modeling again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c50b9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
